{
  
    
        "post0": {
            "title": "Introducing Noisy Imagenette",
            "content": "Introduction . In 2019, fast.ai released the Imagenette and Imagewoof datasets (subsequently updated in 2020), subsets of Imagenet for rapid experimentation and prototyping. It can serve as a small dataset proxy for the ImageNet, or a dataset with more complexity than MNIST or CIFAR10 but still small and simple enough for benchmarking and rapid experimentation. This dataset has been used to test and establish new training techniques like Mish activation function and Ranger optimizer (see here). The dataset also has been used in various papers (see here, here, here, here, here, and here). Clearly, this dataset has been quite useful to machine learning researchers and practitioners for testing and comparing new methods. . A recent hot topic has been the development of various noisy label training techniques. These include novel loss functions like Bi-Tempered Logistic LossTaylor Cross Entropy Loss, or Symmetric Cross Entropy. Additionally, there are many novel training techniques that have been recently developed like MentorMix, DivideMix, Early-Learning Regularization and Noise-Robust Contrastive Learning. Most of these papers are using CIFAR10 and related datasets with synthetically-added noise. Therefore there is an opportunity to develop a dataset similar to Imagenette that allows for rapid prototyping but is complex enough to provide useful results when it comes to noisy label training. . Introducing Noisy Imagenette . We introduce Noisy Imagenette, a version of Imagenette (and Imagewoof) that has synthetically noisy labels at different levels: 1%, 5%, 25%, and 50% incorrect labels. The Noisy Imagenette dataset already comes with the Imagenette dataset: . from fastai.vision.all import * source = untar_data(URLs.IMAGENETTE) . While the regular labels for Imagenette dataset are given as the names of the image folder, the noisy labels are provided as a separate CSV file with columns corresponding to the image filename and labels for each of the different noise levels: . csv_file = pd.read_csv(source/&#39;noisy_imagenette.csv&#39;) csv_file.head() . path noisy_labels_0 noisy_labels_1 noisy_labels_5 noisy_labels_25 noisy_labels_50 is_valid . 0 train/n02979186/n02979186_9036.JPEG | n02979186 | n02979186 | n02979186 | n02979186 | n02979186 | False | . 1 train/n02979186/n02979186_11957.JPEG | n02979186 | n02979186 | n02979186 | n02979186 | n03000684 | False | . 2 train/n02979186/n02979186_9715.JPEG | n02979186 | n02979186 | n02979186 | n03417042 | n03000684 | False | . 3 train/n02979186/n02979186_21736.JPEG | n02979186 | n02979186 | n02979186 | n02979186 | n03417042 | False | . 4 train/n02979186/ILSVRC2012_val_00046953.JPEG | n02979186 | n02979186 | n02979186 | n02979186 | n03394916 | False | . The generation of these noisy labels are provided in this Jupyter notebook. We have also updated fastai&#39;s train_imagenette.py to utilize the new noisy labels. If you want to train on the Noisy Imagenette dataset using this script, just simply pass the --pct-noise argument to the script with the desired noise level. . . Note: The validation set remains clean and its labels are not changed. While technically the accuracy metric is robust to noise, I believe it&#8217;s clearer to use a clean validation set to see if a model is learning appropriate decision boundaries on the right labels. . For the original Imagenette dataset, there are technically (3 image sizes)*(4 number of epoch levels) for both Imagenette and Imagewoof giving a total of 24 leaderboards. If we had each of these 24 leaderboards for the previously mentioned 4 noise levels (1%, 5%, 25%, 50%), that would give us 96 leaderboards! Instead, the Imagenette repository only maintains leaderboards for Noisy Imagenette (and not Imagewoof) for 5% and 50% noise (24 leaderboards). Just like with the regular Imagenette leaderboards, feel free to send a pull request to the Imagenette repository with your results if it beats the current top score. I have provided a baseline which is currently on the leaderboards, as well as a CSV file with the baseline accuracy for all 96 leaderboards. . Backstory . For some background, I started looking into training with label noise because of the recent Cassava Leaf Disease Kaggle Competition (my team was able win a silver medal, see here), which had a really noisy dataset. One of the recent techniques I heard about was SAM, which recently achieved a state-of-the-art score on ImageNet (only to be beaten in a few weeks by techniques/models like Meta Pseudo Labels and NFNets). However, the paper also included some improvements to noisy label training. I had a fastai implementation in-progress for the SAM optimizer (probably will describe in an upcoming blog post) and I wanted to test out its noisy label training capabilities on a dataset. I thought about corrupting the Imagenette labels and use that as my dataset for testing SAM. Jeremy Howard suggested adding it to the main Imagenette dataset and here we are! . Closing Remarks . In conclusion, I hope that this Noisy Imagenette datasets serves as a useful benchmarking dataset for machine learning community when it comes to testing and comparing techniques for training on noisy labels. I hope to experiment with some of these techniques like SAM, the different loss functions, etc. and record those results over on this blog, so be sure to keep an eye on this blog, or follow me on Twitter to get the latest updates! . Acknowledgments . I&#39;d like to thank Jeremy Howard and especially Hamel Husain for adding the Noisy Imagenette dataset and reviewing my blog post. I&#39;d also like to thank Zach Mueller for reviewing my blog post. I&#39;d like to thank Isaac Flath for pointing out an error I originally had when generating the dataset. .",
            "url": "https://tmabraham.github.io/blog/noisy_imagenette",
            "relUrl": "/noisy_imagenette",
            "date": " • Feb 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tmabraham.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tmabraham.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}